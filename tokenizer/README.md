# 分词表Tokenizer

我们透过Google的分词器模块[sentencepiece](https://github.com/google/sentencepiece/tree/master)自定义了一个拥有30000个词汇量的分词表。
